---
title: "A crash course in Bayesian mixed models with brms"
format: 
  revealjs:
    code-overflow: wrap
execute:
  echo: true
  eval: false
---

## What is this class?

- A brief and practical introduction to fitting Bayesian multilevel models in R and Stan
- Using **brms** (**B**ayesian **R**egression **M**odels using **S**tan)
- Quick intro to Bayesian inference
- Mostly practical skills

---

### Minimal prerequisites {.smaller}

- Know what mixed-effects or multilevel model is
- A little experience with stats and/or data science in R
- Vague knowledge of what Bayesian stats are

### Advanced prerequisites

- Knowing about the **lme4** package will help
- Knowing about **tidyverse** and **ggplot2** will help

---

### How to follow the course

- Slides and text version of lessons are online
- Fill in code in the worksheet (replace `...` with code)
- You can always copy and paste code from text version of lesson if you fall behind

---

### Conceptual learning objectives {.smaller}

At the end of this course, you will understand ...

- The basics of Bayesian inference
- What a prior, likelihood, and posterior are
- The basics of how Markov Chain Monte Carlo works
- What a credible interval is

---

### Practical learning objectives {.smaller}

At the end of this course, you will be able to ...

- Write **brms** code to fit a multilevel model with random intercepts and random slopes
- Diagnose and deal with convergence problems
- Interpret **brms** output
- Compare models with LOO information criteria
- Use Bayes factors to assess strength of evidence for effects
- Make plots of model parameters and predictions with credible intervals 

## What is Bayesian inference?

![](https://upload.wikimedia.org/wikipedia/commons/d/d4/Thomas_Bayes.gif)  

## What is Bayesian inference?

A method of statistical inference that allows you to use information you already know to assign a prior probability to a hypothesis, then update the probability of that hypothesis as you get more information

- Used in many disciplines and fields
- We're going to look at how to use it to estimate parameters of statistical models to analyze scientific data
- Powerful, user-friendly, open-source software is making it easier for everyone to go Bayesian

## Bayes' Theorem

![](https://static.scientificamerican.com/blogs/cache/file/5687448D-1F52-4287-A13D53F37A35BE41_source.jpg)

- Thomas Bayes, 1763
- Pierre-Simon Laplace, 1774

## Bayes' Theorem

$$P(A|B) = \frac{P(B|A)P(A)}{P(B)}$$

- How likely an event is to happen based on our prior knowledge about conditions related to that event
- The *conditional* probability of an event *A* occurring, conditioned on the probability of another event *B* occurring

## Bayes' Theorem

$$P(A|B) = \frac{P(B|A)P(A)}{P(B)}$$

The **probability of A being true given that B is true** ($P(A|B)$)  
is equal to the **probability that B is true given that A is true** ($P(B|A)$)  
times the **ratio of probabilities that A and B are true** ($\frac{P(A)}{P(B)}$)

---

### Bayes' theorem and statistical inference

- Let's say $A$ is a statistical model (a hypothesis about the world)
- How probable is it that our hypothesis is true?
- $P(A)$: *prior probability* that we assign based on our subjective knowledge before we get any data

---

### Bayes' theorem and statistical inference

- We go out and get some data $B$
- $P(B|A)$: *likelihood* is the probability of observing that data if our model $A$ is true
- Use the likelihood to update our estimate of probability of our model
- $P(A|B)$: *posterior probability* that model $A$ is true, given that we observed $B$.

---

### Bayes' theorem and statistical inference

$$P(A|B) = \frac{P(B|A)P(A)}{P(B)}$$

- What about $P(B)$?
- *marginal probability*, the probability of the data
- Basically just a normalizing constant
- If we are comparing two models with the same data, the two $P(B)$s cancel out

---

### Restating Bayes' theorem

$$P(model|data) \propto P(data|model)P(model)$$

$$posterior = likelihood \times prior$$

*what we believed before about the world (prior) &times; how much our new data changes our beliefs (likelihood) = what we believe now about the world (posterior)*

## Example

- Find a coin on the street. What is our prior estimate of the probability of flipping heads?
- Now we flip 10 times and get 8 heads. What is our belief now?
- Probably doesn't change much because we have a strong prior and the likelihood of probability = 0.5 is still high enough even if we see 8/10 heads

---

- Shady character on the street shows us a coin and offers to flip it. He will pay \$1 for each tails if we pay \$1 for each heads
- What is our prior estimate of the probability?
- He flips 10 times and gets 8 heads. What's our belief now?

![](https://i.ytimg.com/vi/A-L7KOjyDrE/maxresdefault.jpg)

---

- In classical "frequentist" analysis we cannot incorporate prior information into the analysis
- In each case our point estimate of the probability would be 0.8

## Bayes is computationally intensive

- $P(data|model)$, the likelihood, is needed to get $P(model|data)$, the posterior
- But the "model" is not just one parameter, it might be 100s or 1000s of parameters
- Need to integrate a probability distribution with 100s or 1000s of dimensions
- For many years, this was computationally not possible

## Markov Chain Monte Carlo (MCMC)

- Class of algorithms for sampling from probability distributions
- The longer the chain runs, the closer it gets to the true distribution
- In Bayesian inference, we run multiple Markov chains for a preset number of samples
- Discard the initial samples (warmup)
- What remains is our estimate of the posterior distribution

## Hamiltonian Monte Carlo (HMC) and Stan

![](https://mc-stan.org/images/stan_logo.png)

- HMC is the fastest and most efficient MCMC algorithm that has ever been developed
- It's implemented in software called Stan

## What is brms?

![](https://raw.githubusercontent.com/paul-buerkner/brms/master/man/figures/brms.png)

- An easy way to fit Bayesian mixed models using Stan in R
- Syntax of **brms** models is just like **lme4** 
- Runs a Stan model behind the scenes
- Automatically assigns sensible priors and does lots of tricks to speed up HMC convergence

## Why use Bayes?

- Some models just can't be fit with frequentist maximum-likelihood methods
- Estimate how big effects are instead of yes-or-no framework of rejecting a null hypothesis
- We can say "the probability of something being between a and b is 95%" instead of "if we ran this experiment many times, 95% of the confidence intervals would contain this value."

---

Let's finally fit some Bayesian models!

## Setup

Load packages.

```{r}
library(brms)
library(tidyverse)
library(emmeans)
library(tidybayes)
library(easystats)
```

---

Set plotting theme.

```{r}
theme_set(theme_minimal())
```

Set **brms** options (back-end and cores).

```{r}
options(brms.backend = 'cmdstanr', mc.cores = 4)
```

---

## The data

Read the simulated popularity dataset from CSV on GitHub

```{r}
popular2data <- read_csv('https://github.com/qdread/brms-crash-course/raw/main/data/popular2data.csv')
```

---

### Examine the data

- `pupil`: numeric ID for each individual, nested within each class (between 16 and 26 pupils per class)
- `class`: numeric ID for each classroom (1-100)
- `extrav`: numerical score for extraversion (1-10)
- `sex`: binary variable (0 = boy, 1 = girl)
- `texp`: numerical score for teacher experience level (2-25). Same for all pupils in the same classroom
- `popular`: continuous score (0-10) of the popularity of each individual student (outcome variable)

---

Convert `sex` to a labeled factor

```{r}
popular2data <- mutate(popular2data, sex = factor(sex, labels = c('boy', 'girl')))
```

## Exploratory plots

- Look at relationship between popularity `popular` and extraversion `extrav`
- I will not explain **ggplot2** code for now
- Extraversion values are jittered in x direction

```{r}
(pop_vs_ext <- ggplot(data = popular2data, aes(x = extrav, y = popular)) +
  geom_point(size = 1.2,
             alpha = .8,
             position = position_jitter(width = .2, height = 0)) +
  ggtitle('Popularity vs. extraversion'))
```

---

Add trendline.

```{r}
pop_vs_ext +
  geom_smooth(method = lm, se = FALSE) +
  ggtitle('Popularity vs. extraversion', subtitle = 'overall trendline')
```

---

Take multilevel structure of data into account (color by class).

```{r}
(pop_vs_ext_colored <- ggplot(data = popular2data, aes(x = extrav, y = popular, color = class, group = class)) +
   geom_point(size = 1.2,
              alpha = .8,
              position = position_jitter(width = .2, height = 0)) +
   theme(legend.position = 'none') +
   scale_color_distiller(palette = 'Set1') +
   ggtitle('Popularity vs. extraversion', 'points colored by class'))
```

---

Add class-level trendlines.

```{r}
pop_vs_ext_colored + 
  geom_smooth(method = lm, se = FALSE, linewidth = 0.7, alpha = 0.8) +
  ggtitle('Popularity vs. extraversion', 'trendline by class')
```

---

Highlight classes with most extreme trends.

```{r}
popular2data %>%
  group_by(class) %>%
  mutate(slope = lm(popular ~ extrav)$coefficients[2]) %>%
  ungroup %>%
  mutate(high_or_low = case_when(
    slope >= sort(unique(slope))[98] ~ 'high',
    slope <= sort(unique(slope))[3] ~ 'low',
    TRUE ~ 'mid'
  )) %>%
  ggplot(aes(x = extrav, y = popular, group = class)) +
  geom_point(size = 1.2,
              alpha = .8,
              position = position_jitter(width = .2, height = 0)) +
  geom_smooth(aes(color = high_or_low, size = high_or_low), 
              method = lm, se = FALSE) +
  theme(legend.position = 'none') +
  scale_color_manual(values = c(high = 'blue', low = 'red', mid = 'gray50')) +
  scale_size_manual(values = c(high = 1.2, low = 1.2, mid = 0.6)) +
  ggtitle('Popularity vs. extraversion', 'highlighting 6 classes with most extreme relationship')
```

## Fitting models {.smaller}

For reference this is mixed model syntax from **lme4** package:

```{r}
lmer(popular ~ 1 + (1 | class), data = popular2data)
```

- Dependent or response variable (`popular`) on left side
- Tilde `~` separates dependent from independent variables
- Here the only fixed effect is the global intercept (`1`)
- Random effects specification (`(1 | class)`) has a *design* side (on the left hand) and *group* side (on the right hand) separated by `|`.
- In this case, the `1` on the design side means only fit random intercepts and no random slopes
- `class` on the group side means each class will get its own random intercept

---

### Our first Bayesian multilevel model!

```{r}
fit_interceptonly <- brm(popular ~ 1 + (1 | class),
                         data = popular2data,
                         chains = 2,
                         iter = 200,
                         warmup = 100,
                         init = 'random')
```

- Same formula as **lme4** but with some extra instructions for the HMC sampler
  + Number of Markov chains
  + Iterations for each chain
  + How many iterations to discard as warmup
  + Random initial values
  + No priors specified, so defaults are used
  
---

### Model output

```{r}
summary(fit_interceptonly)
```

- Warning about convergence 
- `Rhat > 1.05` for some parameters
- `Rhat` indicates convergence of MCMC chains, approaching 1 at convergence
- `Rhat < 1.01` is ideal

---

```{r}
plot(fit_interceptonly)
```

Posterior distributions and trace plots for

- the fixed effect intercept (`b_Intercept`)
- the standard deviation of the random class intercepts (`sd_class__Intercept`)
- the standard deviation of the model's residuals (`sigma`)

---

### Dealing with convergence problems

- Warning says either increase iterations or set stronger priors
- Increase iterations to 1000 warmup, 1000 sampling per chain (2000 total)
- `update()` lets us draw more samples without recompiling code

```{r}
fit_interceptonly_moresamples <- update(fit_interceptonly, chains = 2, iter = 2000, warmup = 1000)

plot(fit_interceptonly_moresamples)

summary(fit_interceptonly_moresamples)
```

## Credible intervals

- What is the "95% CI" thing on the parameter summaries?
- *credible interval*, not confidence interval
- more direct interpretation than confidence interval:
  + We are 95% sure the parameter's value is in the 95% credible interval
- based on quantiles of the posterior distribution

---

### Calculating credible intervals

Median and 90% quantile-based credible interval (QI) of the intercept

```{r}
post_samples <- as_draws_df(fit_interceptonly_moresamples)
post_samples_intercept <- post_samples$b_Intercept

median(post_samples_intercept)
quantile(post_samples_intercept, c(0.05, 0.95))
```

> `as_draws_df()` gets all posterior samples for all parameters and puts them into a data frame

---

- Literally anything in a Bayesian model has a posterior distribution, so anything can have a credible interval!
- In frequentist models, you have to do bootstrapping to get that kind of interval on most quantitities

## Variance decomposition

- Proportion of variation at different nested levels
- Calculate the ratio of variance within classes to between classes
- `variance_decomposition()` from **performance** package also gives us a credible interval on the variance ratio

```{r}
variance_decomposition(fit_interceptonly_moresamples, ci = 0.99)
```

---

- Variance ratio is much greater than zero so there is a need for a multilevel model
- But I would recommend using one anyway, if that's the way your study was designed

## Mixed-effects model with first-level predictors

- So far we have only calculated mean of popularity and random variation by class
- But what factors influence popularity of individual students?
- Add first-level predictors (that vary by student) as fixed effects
  + sex
  + extraversion
  
---

- Fixed-effect part of model formula is `1 + sex + extrav`
- No random slope (effect of sex and extraversion on popularity is the same in every class)
- Still using default priors

```{r}
fit_fixedslopes <- brm(popular ~ 1 + sex + extrav + (1 | class),  
                       data = popular2data, 
                       chains = 4, iter = 2000, warmup = 1000,
                       seed = 1400, file = 'fit_fixedslopes')
```

> `seed` sets a random seed for reproducibility, and `file` creates a `.rds` file in the working directory so you can reload the model later without rerunning.

---

```{r}
summary(fit_fixedslopes)
```

- Low Rhat (the model converged)
- Posterior distribution mass for fixed effects is well above zero
- `sigma` (SD of residuals) is smaller than before because we're explaining more variation

## Modifying priors

- `prior_summary()` shows what priors were used to fit the model

```{r}
prior_summary(fit_fixedslopes)
```

- t-distributions on intercept, random effect SD, and residual SD (`sigma`)
- mean of intercept prior is the mean of the data
- mean of the variance parameters is 0 but lower bound is 0 (half bell curves)

---

### Priors on fixed effect slopes

- By default they are flat
- Assigns equal prior probability to *any* possible value
- 0 is as probable as 100000 which is as probable as -55555, etc.
- Not very plausible
- It is OK in this case because the model converged, but often it helps convergence to use priors that only allow "reasonable" values

---

### Refitting with reasonable fixed-effect priors

- `normal(0, 5)` is a good choice
- Mean of 0 means we think that positive effects are just as likely as negative
- SD of 5 means we are still assigning pretty high probability to large effect sizes
- Use `prior()` to assign a prior to each class of parameters

```{r}
fit_fixedslopes_priors <- brm(popular ~ 1 + sex + extrav + (1 | class),  
                       data = popular2data, 
                       prior = c(
                         prior(normal(0, 5), class = b)
                       ),
                       chains = 4, iter = 2000, warmup = 1000,
                       seed = 1450, file = 'fit_fixedslopes_priors')
```

---

```{r}
summary(fit_fixedslopes_priors)
```

- Basically no effect on the results or the performance of the HMC sampler
- But it's something to be mindful of in the future!

---

### Posterior predictive check

- `pp_check()` is a useful diagnostic for how well the model fits the data

```{r}
pp_check(fit_fixedslopes_priors)
```

- black line: density plot of observed data 
- blue lines: density plot of predicted data from 10 random draws from the posterior distribution
- Because these are simulated data, it looks very good in this case

---

### Plotting posterior estimates

- `summary()` only gives us the median and 95% credible interval
- We can work with the full uncertainty distribution (nothing special about 95%)
- Functions from **tidybayes** used to make tables and plots
- `gather_draws()` makes a data frame from the posterior samples of parameters that we choose

```{r}
posterior_slopes <- gather_draws(fit_fixedslopes_priors, b_sex, b_extrav)
```

---

- `median_qi()` gives us median and quantiles of the parameters

```{r}
posterior_slopes %>%
  median_qi(.width = c(.66, .95, .99))
```

---

- Special extensions to **ggplot2** for plotting quantiles of posterior distribution
- I also include a dotted line at zero for comparison

```{r}
ggplot(posterior_slopes, aes(y = .variable, x = .value)) +
  stat_halfeye(.width = c(.8, .95)) +
  geom_vline(xintercept = 0, linetype = 'dashed', linewidth = 1)
```

```{r}
ggplot(posterior_slopes, aes(y = .variable, x = .value)) +
  stat_interval() +
  stat_summary(fun = median, geom = 'point', size = 2) +
  scale_color_brewer(palette = 'Blues') +
  geom_vline(xintercept = 0, linetype = 'dashed', linewidth = 1)
```

## Model with first-level and second-level predictors {.smaller}

- Sex and extraversion vary by individual (first-level predictors)
- Teacher experience `texp` is shared by all individuals in the same classroom (second-level predictor)
- The same syntax is used
- Fixed-effect part is now `1 + sex + extrav + texp`

```{r}
fit_fixed12 <- brm(popular ~ 1 + sex + extrav + texp + (1 | class),  
                   data = popular2data, 
                   prior = c(
                     prior(normal(0, 5), class = b)
                   ),
                   chains = 4, iter = 2000, warmup = 1000,
                   seed = 703, file = 'fit_fixed12')
```

---

Look at trace plots, posterior predictive check, and model summary.

- What do they show?

## Interactions between fixed effects

- You can add interaction terms separated by `:`
- example: `1 + sex + extrav + texp + sex:extrav + extrav:texp`
- Interactions can be within level (like `sex:extrav`) or between levels (like `extrav:texp`)

## Model with random slopes {.smaller}

- So far we've assumed any predictor's effect is the same in every class (only intercept varies, not slope)
- Add *random slope* term to allow both intercept and slope to vary
- Specify a random slope by adding the appropriate slope to the design side of the random effect specification
  + random intercept only `(1 | class)`
  + random intercept and random slope with respect to extraversion `(1 + extrav | class)`
  + random intercept and random slope with respect to extraversion and sex `(1 + extrav + sex | class)`
- Doesn't make sense to have a random slope for `texp` because all students in the same class have the same `texp`

---

```{r}
fit_randomslopes <- brm(popular ~ 1 + sex + extrav + texp + (1 + extrav + sex | class),  
                        data = popular2data, 
                        prior = c(
                          prior(normal(0, 5), class = b)
                        ),
                        chains = 4, iter = 2000, warmup = 1000,
                        seed = 709, file = 'fit_randomslopes')
```

> Note: this model may take a minute or two to sample

- Look at trace plots, `pp_check`, and model summary

---

- Standard deviation of random slopes for sex is very small
- We can probably omit the random slope for sex from the model
- To confirm, plot class-level slopes for sex (sum of fixed + random) with QI intervals

```{r}
sex_slopes <- spread_draws(fit_randomslopes, b_sexgirl, r_class[class,variable]) %>%
  filter(variable == 'sexgirl') %>%
  mutate(slope = b_sexgirl + r_class)

sex_slopes %>% 
  median_qi(slope, .width = c(.66, .90, .95)) %>%
  ggplot(aes(y = class, x = slope, xmin = .lower, xmax = .upper)) +
  geom_interval() +
  geom_point(size = 2) +
  scale_color_brewer(palette = 'Blues')
```

---

- The class-level effects of sex are all very similar
- Refit the model without the random slope for sex, only for extraversion

```{r}
fit_extravrandomslope <- brm(popular ~ 1 + sex + extrav + texp + (1 + extrav | class),  
                             data = popular2data, 
                             prior = c(
                               prior(normal(0, 5), class = b)
                             ),
                             chains = 4, iter = 2000, warmup = 1000,
                             seed = 206, file = 'fit_extravrandomslope')
```

## Comparing models with information criteria {.smaller}

- Leave-one-out (LOO) cross-validation compares models
- How well does a model fit to all data points but one predict the one remaining data point?
- First use `add_criterion()` to compute the LOO criterion for each model
- Then use `loo_compare()` to rank the models

```{r}
fit_interceptonly_moresamples <- add_criterion(fit_interceptonly_moresamples, 'loo')
fit_fixedslopes_priors <- add_criterion(fit_fixedslopes_priors, 'loo')
fit_fixed12 <- add_criterion(fit_fixed12, 'loo')
fit_randomslopes <- add_criterion(fit_randomslopes, 'loo')
fit_extravrandomslope <- add_criterion(fit_extravrandomslope, 'loo')
```

---

```{r}
loo_compare(fit_interceptonly_moresamples, fit_fixedslopes_priors, fit_fixed12, fit_randomslopes, fit_extravrandomslope)
```

- Models ranked by ELPD (expected log pointwise predictive density)
- The best one always has 0 and the others are ranked relative to it
- The random slope models do equally well, so we can prefer the simpler one
- The random intercept models do quite a bit worse
- The model with no fixed effects does very poorly

## Assessing evidence with Bayes Factors

- Bayesian analogue of a p-value
- Ratio of evidence between two models: $\frac{P(model_1|data)}{P(model_2|data)}$
- Ranges from 0 to infinity
- BF = 1 means equal evidence, BF > 1 means more evidence for model 1
- No "significance" threshold but BF > 10 is usually called strong evidence

---

### Bayes factors for each parameter {.smaller}

- R package **bayestestR** lets us compute BF for each parameter
- Ratio of evidence for posterior : evidence for prior
- Our prior distributions were centered at 0 so they are like "null hypotheses"
- BF = 1 means we did not change our belief about the parameter at all from the prior, after seeing the data
- BF > 1 means we've changed our belief about the parameter after seeing the data
- BF < 1 means we have even stronger evidence that the prior is true, after seeing the data

---

```{r}
bayesfactor_parameters(fit_extravrandomslope)
```

- BF < 1 for the intercept
- BF >> 1000 for all other fixed effects
- **WARNING**: BFs are very sensitive to your choice of prior

## Making prediction plots

- To finish, here are some examples of plots you can make to visualize model output
- Posterior expected values given different values of the predictors

---

- `conditional_effects()` is a quick way to plot all fixed effects with 95% credible intervals

```{r}
conditional_effects(fit_extravrandomslope)
```

---

- Customized plots can be made with packages **emmeans** and **tidybayes**
- `emmeans()` gives you the expected value at specific levels of fixed predictors

```{r}
sex_emmeans <- emmeans(fit_extravrandomslope, ~ sex)

gather_emmeans_draws(sex_emmeans) %>%
  ggplot(aes(x = .value, y = sex)) +
  stat_interval(.width = c(.66, .95, .99)) +
  stat_summary(fun = median, geom = 'point', size = 2) +
  scale_color_brewer(palette = 'Greens') +
  ggtitle('posterior expected value by sex', 'averaged across extraversion and teacher experience')
```

---

- `add_epred_draws()` gives you the expected value at levels of categorical and continuous fixed effects
- Trend of popularity versus extraversion for each sex, at average value of `texp`
- Averages across class-level random effects

```{r}
expand_grid(texp = mean(popular2data$texp),
            extrav = seq(1, 10, by = .5),
            sex = c('boy', 'girl')) %>%
  add_epred_draws(fit_extravrandomslope, re_formula = ~ 0) %>%
  ggplot(aes(x = extrav, y = .epred, group = sex, color = sex)) +
  stat_lineribbon(.width = c(.66, .95, .99)) +
  scale_fill_grey() +
  scale_color_manual(values = c(boy = 'blue', girl = 'pink')) +
  labs(y = 'posterior expected value of popularity') +
  ggtitle('posterior expected value by extraversion + sex', 'at average value of teacher experience')
```

---

- We can include the class-level random effects as well if we want
- This plot only shows 10 of the 100 classes

```{r}
expand_grid(texp = mean(popular2data$texp),
            extrav = seq(1, 10, by = .5),
            sex = c('boy', 'girl'),
            class = 1:10) %>%
  add_epred_draws(fit_extravrandomslope, re_formula = ~ (1 + extrav|class)) %>%
  ggplot(aes(x = extrav, y = .epred, group = class, color = sex)) +
  facet_wrap(~ sex) +
  stat_lineribbon(.width = c(.95), alpha = 1/4) +
  scale_color_manual(values = c(boy = 'blue', girl = 'pink')) +
  labs(y = 'posterior expected value of popularity') +
  ggtitle('class-level predictions of popularity vs. extraversion by sex', 'at average value of teacher experience')
```

## Conclusion

What did we learn? Let's revisit the learning objectives!

## Conceptual learning objectives

You now understand...

- The basics of Bayesian inference
- Definition of prior, likelihood, and posterior 
- How Markov Chain Monte Carlo works
- What a credible interval is

## Practical skills

You now can...

- Write **brms** code to fit a multilevel model with random intercepts and random slopes
- Diagnose and deal with convergence problems
- Interpret **brms** output
- Compare models with LOO information criteria
- Use Bayes factors to assess strength of evidence for effects
- Make plots of model parameters and predictions with credible intervals 

---

Congratulations, you are now Bayesians!

---

- See text version of lesson for further reading and useful resources
- Please send any feedback to `quentin.read@usda.gov`!