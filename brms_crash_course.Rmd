---
title: "title"
author: "Quentin D. Read"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: 
  html_document:
    toc: true
    toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

# Introduction

What is this course? It is a crash course in **brms**: a brief and practical introduction to fitting Bayesian multilevel models in R and Stan using the [**brms**](https://paul-buerkner.github.io/brms/) package (**B**ayesian **R**egression **M**odels using **S**tan). Because this is intended as a practical skills course, I will only give a quick and surface-level description of Bayesian analysis and how it can be used for multilevel regression models.

## What should you know coming into this course?

This course is designed for practicing researchers who have some experience with statistical analysis and coding in R.

### Minimal prerequisites

- You should have at least heard of a mixed model or multilevel model.
- You should have at least tried to do stats and/or data science in R before.
- You should have at least a vague knowledge of what Bayesian analysis and inference are.

### Advanced prerequisites

- If you have experience with the R package **lme4**, that's great because the syntax of the R package **brms** is modeled after **lme4**.
- If you have experience with the R **tidyverse** packages, including **ggplot2**, for manipulating and plotting data, that's also great because we will be using them in this course.

If you don't know what those packages are, don't sweat it because you can just follow along by copying the code.

## What will you learn from this course?

### Conceptual learning objectives

At the end of this course, you will understand ...

- The very basics of Bayesian inference
- What a prior, likelihood, and posterior are in the context of a Bayesian model
- The very basics of how Markov Chain Monte Carlo works
- What a credible interval is

### Practical skills

At the end of this course, you will be able to ...

- Write the code to specify and sample a multilevel model with random intercepts, random slopes, and interactions in **brms**.
- Generate plots and statistics to make sure your model converged, and know a few ways to deal with convergence problems.
- Interpret the default output of a multilevel model in **brms**.
- Use the posterior distributions of parameters from a **brms** model to generate predictions and make contrasts.
- Make tables of point estimates and credible intervals of parameters and predicted values from the model.
- Make plots of model parameters and predictions, with credible intervals to show uncertainty.

## What is Bayesian inference?

In its simplest form, Bayesian inference is xxxx

Laplace and Reverend Bayes independently invented it

## Bayes' Theorem

xxxx

## Example

Let's say we pick up a coin off the street. We have a very strong prior belief that the true probability of flipping heads on any given flip is 0.5. Let's say we flipped the coin 10 times and got 8 heads. We probably wouldn't change that belief very much based on that evidence. It simply isn't enough evidence to sway us off that belief. 

But let's say a shady character on the street corner approaches us, shows us a coin, and offers to flip the coin 10 times. He offers to pay us \$1 for every tails if we pay him \$1 for every heads. Most likely, before we even got any new evidence, we'd be likely to think that the true probability of heads is not exactly 0.5. Or at least we would have a more skeptical prior belief, and we would be willing to entertain a more 

In a classical, "frequentist" analysis, there is no formal way to incorporate prior information into a statistical model. In both cases, we have no choice but to say the point estimate of the true probability of heads is 0.8. 

## Why is it so computationally intensive?

The main reason that Bayesian statistics was not widely adopted initially is probably a practical reason. It boils down to `p(data|model)`, the likelihood. This is a xxxx.

## What is Stan?

Andrew Gelman and colleagues developed

Older methods for Monte Carlo sampling were not nearly as efficient as Hamiltonian Monte Carlo. 

## What is brms?

Paul Buerkner developed

## Why use it?

- Philosophical reasons
- Math reasons

# Setup

You should have already installed the R packages **brms** and **cmdstanr**, and used **cmdstanr** to install CmdStan on your personal computer. We will also be using the **tidyverse**, **emmeans**, and **tidybayes** packages today.

```{r load packages}
library(brms)
library(tidyverse)
library(emmeans)
library(tidybayes)
library(easystats)
```

Set a plotting theme.

```{r ggplot theme}
theme_set(theme_minimal())
```

Set **brms** options. The option `brms.backend = 'cmdstanr'` means we want to use CmdStanR as a backend to run the **brms** models, instead of the default backend. If you were unable to install CmdStan correctly, omit the `brms.backend = 'cmdstanr'` part. The option `mc.cores = 4` means we will use four processor cores in parallel to fit the models, meaning we can allow four Markov chains to sample simultaneously.

```{r}
options(brms.backend = 'cmdstanr', mc.cores = 4)
```

## The data

Read the data CSV to a data frame. It is hosted on the GitHub repository for this course.

```{r}
popular2data <- read_csv('https://github.com/qdread/brms-crash-course/raw/main/data/popular2data.csv')
```

Examine the data. These are simulated (i.e., fake) data that could have come from a study exploring what characteristics predict popularity of a student.

```{r}
glimpse(popular2data)
```

There are quite a few columns but we only need to be concerned with a few:

- `pupil`: a numeric ID for each individual (1-26). Note that the IDs are "recycled" within each class. For example pupil 1 in class 1 is not the same person as pupil 1 in class 2.
- `class`: a numeric ID for each class (1-100). 
- `extrov`: a numerical score for extroversion, or how outgoing each individual is (1-10).
- `sex`: a binary variable (0 = boy, 1 = girl).
- `texp`: a numerical score for teacher experience level (2-25).
- `popular`: a continuous score (0-10) of the popularity of each individual student. This is the outcome variable we are interested in explaining. 

In studies of this kind, the popularity score is obtained by having each student in the class rate all other students on their popularity, then averaging the score each student received. Therefore we might expect there to be a strong effect of class on the mean popularity rating, as well as the relationship between extroversion and popularity. Thus this is an ideal situation for a multilevel model! (obviously, because we simulated this fake dataset to teach you about multilevel models...)

# Exploratory plots

Let's take a look at the relationship between extroversion and popularity. (I won't explain the **ggplot2** code in this lesson.) In this plot, the points are jittered in the x direction for plotting purposes because extroversion only has integer values.

```{r}
(pop_vs_ext <- ggplot(data = popular2data, aes(x = extrov, y = popular)) +
  geom_point(size = 1.2,
             alpha = .8,
             position = position_jitter(width = .2, height = 0)) +
  ggtitle('Popularity vs. Extroversion'))
```

Right now, this completely ignores the multilevel structure of the dataset. The popularity scores of pupils from the same class are not independent of one another. So far, ignoring that wrinkle, we can use the eyeball test to see that there is a moderately strong positive relationship between popularity and extroversion.

We can add a regression trendline to the plot. This is a simple least squares linear regression line that ignores the nestedness of pupils within classes. Plotting simplified models in this way is fine to guide the eye for exploratory purposes.

```{r}
pop_vs_ext +
  geom_smooth(method = lm, se = FALSE) +
  ggtitle('Popularity vs. Extroversion', subtitle = 'overall trendline')
```

This further supports our intuition that more outgoing students tend to be more popular.

Now let's start to take the nested structure of the data into account. Color the points in the scatterplot based on which class (from 1 to 100) each pupil is in.

```{r}
(pop_vs_ext_colored <- ggplot(data = popular2data, aes(x = extrov, y = popular, color = class, group = class)) +
   geom_point(size = 1.2,
              alpha = .8,
              position = position_jitter(width = .2, height = 0)) +
   theme(legend.position = 'none') +
   scale_color_distiller(palette = 'Set1') +
   ggtitle('Popularity vs. Extroversion', 'points colored by class'))
```

It's hard to visually separate the classes, because there are 100 of them and they largely overlap. Let's add different regression lines for each class. This is equivalent to fitting a separate linear model to each class. 

```{r}
pop_vs_ext_colored + 
  geom_smooth(method = lm, se = FALSE, linewidth = 0.7, alpha = 0.8) +
  ggtitle('Popularity vs. Extroversion', 'trendline by class')
```

You can see that the relationship between popularity and extroversion varies by class -- but it's positive on average. A multilevel model helps us estimate the overall trend while taking into account the variation in trend (slope and intercept) between classes.

Here is a final exploratory plot that highlights the classes with the most extreme trends. The trendlines classes with the three highest slopes are plotted in blue, and the three lowest slopes in red. (This requires writing some code to pre-calculate the slopes and then modifying the plotting code to change the colors of the lines with the most extreme slopes).

```{r}
popular2data %>%
  group_by(class) %>%
  mutate(slope = lm(popular ~ extrov)$coefficients[2]) %>%
  ungroup %>%
  mutate(high_or_low = case_when(
    slope >= sort(unique(slope))[98] ~ 'high',
    slope <= sort(unique(slope))[3] ~ 'low',
    TRUE ~ 'mid'
  )) %>%
  ggplot(aes(x = extrov, y = popular, group = class)) +
  geom_point(size = 1.2,
              alpha = .8,
              position = position_jitter(width = .2, height = 0)) +
  geom_smooth(aes(color = high_or_low, size = high_or_low), 
              method = lm, se = FALSE) +
  theme(legend.position = 'none') +
  scale_color_manual(values = c(high = 'blue', low = 'red', mid = 'gray50')) +
  scale_size_manual(values = c(high = 1.2, low = 1.2, mid = 0.6)) +
  ggtitle('Popularity vs. Extroversion', 'highlighting 6 classes with most extreme relationship')
```

# Fitting models

## Intercept-only model

If you have ever fit a mixed model using the R package `lme4`, this will look familiar to you:

```{r, eval = FALSE}
lmer(popular ~ 1 + (1 | class), data = popular2data)
```

That is the syntax for fitting a mixed-effects model with the only fixed effect being the intercept (`1`) and a random intercept fit to each class. In the `lme4` package, the random effects specification has a *design* side (on the left hand) and *group* side (on the right hand) separated by `|`. In this case, `(1 | class)`, the `1` on the design side means only fit random intercepts and no random slopes, and the `class` on the group side means the groups that each get their own intercept are defined by the `class` variable.

The beauty of **brms** is that it allows you to fit a Bayesian multilevel model with very similar syntax to the frequentist **lme4**. Instead of getting maximum likelihood point estimates of the model parameters (the fixed and random effects), we're going to get posterior distributions for all of them! Get hyped!!! 

But to do this we need to give a few extra directions to the Hamiltonian Monte Carlo sampling algorithm:

- How many chains should run?
- How many iterations should each chain run in total?
- How many iterations should be used for the initial warmup phase (these are not included in the final posterior distribution)?
- What are the initial values for the parameters?

If we don't specify any of these, **brms** will use default values for number of chains and iterations, and it will assign random values for the initial values. 

You may notice I have made no mention of priors yet. Another great thing about **brms** is that if you do not specify priors, it will assign uninformative priors that are at least plausible for each of the parameters. (For example, if the parameter is a variance parameter that must be positive, it will assign a prior distribution that is truncated at zero). This can be a great time saver but it can also be a dangerous thing. We will revisit priors later. For now we will skip specifying the priors and use the defaults.

Without further ado, here is our first Bayesian multilevel model!

```{r}

```


# Resources

As I've already mentioned, this is not intended to be a replacement for a full Bayesian course. Richard McElreath has written an amazing Bayesian course called [Statistical Rethinking](https://xcelab.net/rm/statistical-rethinking/). It has a print book and ebook version, as well as a set of free video lectures. Solomon Kurz translated all the code examples in the book to **brms**, **ggplot2**, and **tidyverse** code: [Statistical Rethinking Recoded](https://bookdown.org/ajkurz/Statistical_Rethinking_recoded/). Both are highly recommended.

Here are some links to tutorials and lessons for learning basic Bayesian statistics with **brms**.

- [Building a multilevel model in brms](https://www.rensvandeschoot.com/tutorials/brms-started/) by Rens van de Schoot
- [An introduction to Bayesian multilevel models using brms](https://www.barelysignificant.com/phd_thesis/appendix-brms.html) by Ladislas Nalborczyk

Paul Buerkner published two papers on **brms**:

- [Buerkner 2017, Journal of Statistical Software](https://www.jstatsoft.org/article/view/v080i01)
- [Buerkner 2018, The R Journal](https://journal.r-project.org/archive/2018/RJ-2018-017/index.html)

# Alternatives

- rstanarm
- MCMCglmm
- PyStan (other platforms for Stan)
- CmdStan