Imagine you have prior knowledge that about 1 in 1000 people in the population is a werewolf, and you have a werewolf detector. The detector never gives false negatives (it will correctly identify a werewolf 100% of the time), and it will also correctly identify that someone is not a werewolf 99% of the time. Now imagine you point your werewolf detector at the mailman and it screams "DANGER! WEREWOLF DETECTED!" Use Bayes' Theorem to determine the posterior probability that the mailman is a werewolf.

*Hint*: Here $A$ is "the mailman is a werewolf" and $B$ is "the werewolf detector tested positive". So $P(A|B)$ is the probability that the mailman is a werewolf, given that the werewolf detector tested positive. That's what we want to know. $P(B|A)$ is the probability that the werewolf test is positive given that the mailman is a werewolf. We know this is 0.95. $P(A)$ is the prior probability that the mailman is a werewolf, before we get the data (the positive werewolf test). Based on our background knowledge that's 1 in 1000 or 0.001. Finally $P(B)$ is the probability that the werewolf detector tested positive. 

# posterior predictive checks that look good and bad

```{r}
library(brms)
options(brms.backend = 'cmdstanr')
set.seed(111)

d <- data.frame(x = rlnorm(1000))

fit <- brm(x ~ 1, data = d, family = gaussian())

pp_check(fit)
```

# trace plots that look good and bad

```{r}
library(brms)
library(ggplot2)
options(brms.backend = 'cmdstanr', mc.cores = 4)
theme_set(theme_bw())

set.seed(123)

a <- expand.grid(
  g1 = letters[1:3],
  g2 = LETTERS[1:3],
  x1 = 1:10
 )

a$y <- rnorm(90, 0, 5) + a$x1
a$x2 <- 3 * a$x1

fit <- brm(y ~ x1 + x2 + x1:x2 + (1+x1+x2|g1) + (1+x1+x2|g2), data = a, chains = 4, warmup = 10, iter = 1010, seed = 1, file = 'ex2bad')
fitbetter <- brm(y ~ x1 + (1+x1|g1) + (1+x1|g2), data = a, chains = 4, warmup = 1000, iter = 2000, seed = 1, file = 'ex2good')

plot(fit, pars = 'b_x1')
plot(fitbetter, pars = 'b_x1')
```


